seed: 42
experiment_name: "default"
steps_per_model_checkpoint: 10

env_config:
  env:
    num_envs: 4096
    num_observations: 235
    num_actions: 12
    env_spacing: 3
    send_timeouts: True
    episode_length_s: 20

  terrain: null

  init_state: null

  control:
    decimation: 4

  asset: null

  normalization:
    obs_scales: null

  noise:
    add_noise: True
    noise_level: 1.0
    noise_scales: null

  domain_rand: null

  rewards:
    scales: null
    only_positive_rewards: False

train_config:
  policy:
    init_noise_std: 1.0
    actor_hidden_dims: [512, 256, 128]
    critic_hidden_dims: [512, 256, 128]
    activation: 'elu'
  algorithm:
    value_loss_coef: 1.0
    use_clipped_value_loss: True
    clip_param: 0.2
    entropy_coef: 0.01
    num_learning_epochs: 5
    num_mini_batches: 4  # mini batch size = num_envs*nsteps / nminibatches
    learning_rate: 1.e-3  # 5.e-4
    schedule: 'adaptive'  # could be adaptive, fixed
    gamma: 0.99
    lam: 0.95
    desired_kl: 0.01
    max_grad_norm: 1.
  runner:
    policy_class_name: 'ActorCritic'
    algorithm_class_name: 'PPO'
    num_steps_per_env: 24  # per iteration
    max_iterations: 1500  # number of policy updates
    # logging
    save_interval: 50  # check for potential saves every this many iterations
    experiment_name: 'test'
    run_name: ''
    # load and resume
    resume: False
    load_run: -1  # -1: last run
    checkpoint: -1  # -1 = last saved model
    resume_path: None  # updated from load_run and chkpt
