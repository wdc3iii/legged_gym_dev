{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395a2a6b",
   "metadata": {},
   "source": [
    "# Tube Learning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455d9ab",
   "metadata": {},
   "source": [
    "## DataFrame Construction"
   ]
  },
  {
   "cell_type": "code",
   "id": "4bec8682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T20:40:58.944792Z",
     "start_time": "2024-07-09T20:40:58.823189Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import glob"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b790888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             z  \\\n",
      "0  [0.015644509840537696, 0.01509342525270101]   \n",
      "1   [0.03128901968107539, 0.03018685050540202]   \n",
      "2  [0.04693352952161309, 0.045280275758103034]   \n",
      "3   [0.06257803936215078, 0.06037370101080404]   \n",
      "4   [0.07822254920268848, 0.07546712626350505]   \n",
      "\n",
      "                                          v  \\\n",
      "0  [0.7822254920268847, 0.7546712626350505]   \n",
      "1  [0.7822254920268847, 0.7546712626350505]   \n",
      "2  [0.7822254920268847, 0.7546712626350505]   \n",
      "3  [0.7822254920268847, 0.7546712626350505]   \n",
      "4  [0.7822254920268847, 0.7546712626350505]   \n",
      "\n",
      "                                            w_xy         w  \\\n",
      "0   (-0.00878550348036447, -0.02359656427171346)  0.025179   \n",
      "1  (-0.015280707798460302, -0.04643722057574027)  0.048887   \n",
      "2  (-0.022484283517389264, -0.06901448954244008)  0.072585   \n",
      "3  (-0.028610522045809383, -0.08874247581229673)  0.093240   \n",
      "4   (-0.03388549635077312, -0.10423396509719474)  0.109604   \n",
      "\n",
      "                                         w_xy_p1    w_t_p1  \n",
      "0   (-0.00878550348036447, -0.02359656427171346)  0.025179  \n",
      "1  (-0.015280707798460302, -0.04643722057574027)  0.048887  \n",
      "2  (-0.022484283517389264, -0.06901448954244008)  0.072585  \n",
      "3  (-0.028610522045809383, -0.08874247581229673)  0.093240  \n",
      "4   (-0.03388549635077312, -0.10423396509719474)  0.109604  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "local_folder = 'rom_tracking_data/fxqv2jkf'\n",
    "\n",
    "# Load the pickle file\n",
    "with open(f\"{local_folder}/dataset.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Create DataFrame from the pickle data\n",
    "df = pd.DataFrame({\n",
    "    'z': list(data['z']),\n",
    "    'v': list(data['v']),\n",
    "    'pz_x': list(data['pz_x']),\n",
    "    'z_p1': list(data['z_p1']),\n",
    "    'pz_x_p1': list(data['pz_x_p1']),\n",
    "})\n",
    "\n",
    "# Compute the vector differences for w_xy_t and its norm w_t\n",
    "df['w_xy'] = [tuple(np.array(pz) - np.array(z)) for pz, z in zip(df['pz_x'], df['z'])]\n",
    "df['w'] = [np.linalg.norm(np.array(pz) - np.array(z)) for pz, z in zip(df['pz_x'], df['z'])]\n",
    "\n",
    "# Compute the vector differences for the next timestep w_xy_{t+1} and its norm w_{t+1}\n",
    "df['w_xy_p1'] = [tuple(np.array(pz) - np.array(z)) for pz, z in zip(df['pz_x_p1'], df['z_p1'])]\n",
    "df['w_t_p1'] = [np.linalg.norm(np.array(pz) - np.array(z)) for pz, z in zip(df['pz_x_p1'], df['z_p1'])]\n",
    "\n",
    "# Cleanup: Drop the intermediate columns if they are no longer needed\n",
    "df.drop(['pz_x', 'z_p1', 'pz_x_p1'], axis=1, inplace=True)\n",
    "\n",
    "# Print to check the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "id": "47fc7dd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T20:40:58.948222Z",
     "start_time": "2024-07-09T20:40:58.945505Z"
    }
   },
   "source": [
    "# Function to safely evaluate lists\n",
    "def safe_eval(col):\n",
    "    try:\n",
    "        return ast.literal_eval(col)\n",
    "    except ValueError:\n",
    "        return col  # Return as is if it's not a string representation of a list\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Set the number of robots you want to include\n",
    "num_robots = 5  # Set the number of robots to include\n",
    "robot_indices = list(range(num_robots))  # Generates a list [0, 1, 2, ..., num_robots-1]\n",
    "\n",
    "# Use glob to find all the files that match the pattern\n",
    "file_list = glob.glob('data/trajectory_data_*.csv')\n",
    "\n",
    "# Loop through the files sorted to maintain the order\n",
    "for filename in sorted(file_list):\n",
    "    temp_df = pd.read_csv(filename)\n",
    "    # Filter the DataFrame to only include rows where the robot_index is in the list of desired indices\n",
    "    temp_df = temp_df[temp_df['robot_index'].isin(robot_indices)]\n",
    "    # Apply transformations right after reading\n",
    "    temp_df['joint_positions'] = temp_df['joint_positions'].apply(safe_eval)\n",
    "    temp_df['joint_velocities'] = temp_df['joint_velocities'].apply(safe_eval)\n",
    "    all_data = pd.concat([all_data, temp_df], ignore_index=True)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b7d36b66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T20:40:59.229999Z",
     "start_time": "2024-07-09T20:40:58.948610Z"
    }
   },
   "source": [
    "# Now create the derived columns\n",
    "all_data['x_t'] = all_data.apply(lambda row: row['joint_positions'] + row['joint_velocities'], axis=1)\n",
    "all_data['u_t'] = all_data.apply(lambda row: [row['velocity_x'], row['velocity_y']], axis=1)\n",
    "all_data['z_t'] = all_data.apply(lambda row: [row['traj_x'], row['traj_y']], axis=1)\n",
    "all_data['v_t'] = all_data.apply(lambda row: [row['reduced_command_x'], row['reduced_command_y']], axis=1)\n",
    "\n",
    "# Calculate the vector differences without norm for w_xy_t and shift it for w_xy_{t+1}\n",
    "all_data['w_xy_t'] = all_data.apply(lambda row: [row['position_x'] - row['traj_x'], row['position_y'] - row['traj_y']], axis=1)\n",
    "all_data['group'] = all_data['episode_number'].astype(str) + '_' + all_data['robot_index'].astype(str)\n",
    "\n",
    "# Original calculation for w_t for reference\n",
    "all_data['w_t'] = np.sqrt((all_data['position_x'] - all_data['traj_x'])**2 + (all_data['position_y'] - all_data['traj_y'])**2)\n",
    "\n",
    "# Shift operations for next timestep values\n",
    "all_data['x_{t+1}'] = all_data.groupby('group')['x_t'].shift(-1)\n",
    "all_data['z_{t+1}'] = all_data.groupby('group')['z_t'].shift(-1)\n",
    "all_data['w_{t+1}'] = all_data.groupby('group')['w_t'].shift(-1)\n",
    "all_data['w_xy_{t+1}'] = all_data.groupby('group')['w_xy_t'].shift(-1)\n",
    "\n",
    "# Function to drop the first and last 10 data points from each episode\n",
    "def drop_edges(group):\n",
    "    return group.iloc[10:-10]\n",
    "all_data = all_data.groupby('group', group_keys=False).apply(drop_edges)\n",
    "\n",
    "# Drop rows where x_{t+1}, z_{t+1}, w_{t+1}, and w_xy_{t+1} do not exist\n",
    "all_data.dropna(subset=['x_{t+1}', 'z_{t+1}', 'w_{t+1}', 'w_xy_{t+1}'], inplace=True)\n",
    "\n",
    "# Select and order the final columns\n",
    "final_df = all_data[['group', 'x_t', 'u_t', 'z_t', 'v_t', 'w_t', 'w_xy_t', 'x_{t+1}', 'z_{t+1}', 'w_{t+1}', 'w_xy_{t+1}']]\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set a DataFrame with multiple columns to the single column x_t",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Now create the derived columns\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mall_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mx_t\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m all_data\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m row: row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjoint_positions\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjoint_velocities\u001B[39m\u001B[38;5;124m'\u001B[39m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      3\u001B[0m all_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu_t\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m all_data\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m row: [row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvelocity_x\u001B[39m\u001B[38;5;124m'\u001B[39m], row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvelocity_y\u001B[39m\u001B[38;5;124m'\u001B[39m]], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      4\u001B[0m all_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mz_t\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m all_data\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m row: [row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraj_x\u001B[39m\u001B[38;5;124m'\u001B[39m], row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraj_y\u001B[39m\u001B[38;5;124m'\u001B[39m]], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/legged_gym_dev/lib/python3.8/site-packages/pandas/core/frame.py:3940\u001B[0m, in \u001B[0;36mDataFrame.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   3938\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setitem_array(key, value)\n\u001B[1;32m   3939\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, DataFrame):\n\u001B[0;32m-> 3940\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_item_frame_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3941\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m (\n\u001B[1;32m   3942\u001B[0m     is_list_like(value)\n\u001B[1;32m   3943\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mis_unique\n\u001B[1;32m   3944\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_indexer_for([key])) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(value)\n\u001B[1;32m   3945\u001B[0m ):\n\u001B[1;32m   3946\u001B[0m     \u001B[38;5;66;03m# Column to set is duplicated\u001B[39;00m\n\u001B[1;32m   3947\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setitem_array([key], value)\n",
      "File \u001B[0;32m~/miniforge3/envs/legged_gym_dev/lib/python3.8/site-packages/pandas/core/frame.py:4094\u001B[0m, in \u001B[0;36mDataFrame._set_item_frame_value\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   4091\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   4093\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(value\u001B[38;5;241m.\u001B[39mcolumns) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m-> 4094\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   4095\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot set a DataFrame with multiple columns to the single \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4096\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumn \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4097\u001B[0m     )\n\u001B[1;32m   4099\u001B[0m \u001B[38;5;28mself\u001B[39m[key] \u001B[38;5;241m=\u001B[39m value[value\u001B[38;5;241m.\u001B[39mcolumns[\u001B[38;5;241m0\u001B[39m]]\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot set a DataFrame with multiple columns to the single column x_t"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "73efb734",
   "metadata": {},
   "source": [
    "We have $D=\\{\\omega_t, x_t, u_t, z_t, v_t, \\omega_{t+1}, x_{t+1}, z_{t+1}\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "id": "30dbdf51",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T20:40:59.230663Z",
     "start_time": "2024-07-09T20:40:59.230616Z"
    }
   },
   "source": [
    "# Print the final DataFrame\n",
    "final_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5d3770d",
   "metadata": {},
   "source": [
    "Optional Saving:"
   ]
  },
  {
   "cell_type": "code",
   "id": "cdcd6e5c",
   "metadata": {},
   "source": [
    "final_df.to_csv('processed_trajectory_data.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d689932",
   "metadata": {},
   "source": [
    "## Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "id": "df2ec200",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.login(key=\"70954bb73c536b7f5b23ef315c7c19b511e8a406\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7b6799e",
   "metadata": {},
   "source": [
    "Load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5abf44b7",
   "metadata": {},
   "source": [
    "def safe_eval(col):\n",
    "    try:\n",
    "        return ast.literal_eval(col)\n",
    "    except ValueError:\n",
    "        return col  # Return as is if it's not a string representation of a list\n",
    "\n",
    "def convert_to_tensor_input(row):\n",
    "    flat_list = []\n",
    "    for item in row:\n",
    "        if isinstance(item, list):\n",
    "            flat_list.extend(item)\n",
    "        else:\n",
    "            flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "def load_and_prepare_data(filename, tube_type):\n",
    "    df = pd.read_csv(filename)\n",
    "    list_columns = ['u_t', 'z_t', 'v_t'] + (['w_xy_t', 'w_xy_{t+1}'] if tube_type == 'rectangular' else [])\n",
    "    feature_columns = ['u_t', 'z_t', 'v_t']\n",
    "    \n",
    "    for col in list_columns:\n",
    "        df[col] = df[col].apply(safe_eval)  # Assumes safe_eval correctly parses strings into lists\n",
    "\n",
    "    X = torch.tensor(df[feature_columns].apply(convert_to_tensor_input, axis=1).tolist(), dtype=torch.float32)\n",
    "    \n",
    "    if tube_type == 'rectangular':\n",
    "        target_columns = ['w_xy_t', 'w_xy_{t+1}']\n",
    "    else:\n",
    "        target_columns = ['w_t', 'w_{t+1}']\n",
    "\n",
    "    # Construct the target tensor manually from rows\n",
    "    y_data = df[target_columns].apply(lambda row: [row[col] for col in target_columns], axis=1).tolist()\n",
    "    y = torch.tensor(y_data, dtype=torch.float32)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def create_data_loaders(X, y, batch_size=64):\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(X, y[:, 1].unsqueeze(1))  # Assumes the target is at the second position\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a02ce8e4",
   "metadata": {},
   "source": [
    "Model specifications:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f732340",
   "metadata": {},
   "source": [
    "class TubeWidthPredictor(nn.Module):\n",
    "    def __init__(self, input_size, num_units, num_layers, output_dim=1):\n",
    "        super(TubeWidthPredictor, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_size, num_units), nn.ReLU()])\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_units, num_units))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(num_units, output_dim))  # Dynamically set output dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, delta=1.0):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "\n",
    "    def forward(self, y_pred, y_true, tube_type):\n",
    "        if tube_type == 'sphere':\n",
    "            residual = y_true - y_pred\n",
    "            loss = torch.where(residual <= 0, self.alpha * residual, (1 - self.alpha) * residual.abs())\n",
    "            return self.huber(loss, torch.zeros_like(loss))\n",
    "        elif tube_type == 'rectangular':\n",
    "            # Compute L2 norm of the residuals for each component\n",
    "            y_true = y_true.squeeze(1) # because im a lazy idiot\n",
    "            residual_x = y_true[:, 0] - y_pred[:, 0]\n",
    "            residual_y = y_true[:, 1] - y_pred[:, 1]\n",
    "            norm_residual = torch.sqrt(residual_x**2 + residual_y**2)\n",
    "            loss = torch.where(norm_residual <= 0, self.alpha * norm_residual, (1 - self.alpha) * norm_residual.abs())\n",
    "            return self.huber(loss, torch.zeros_like(loss))\n",
    "\n",
    "def train_and_test(model, criterion, optimizer, train_loader, test_loader, tube_type, num_epochs=500):\n",
    "    best_test_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=0, leave=True):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, targets in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets, tube_type)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Log train loss to wandb\n",
    "        wandb.log({'Train Loss': train_loss / len(train_loader), 'Epoch': epoch})\n",
    "\n",
    "        # Evaluate the model\n",
    "        test_loss, metrics = evaluate_model(model, test_loader, criterion, tube_type)\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        wandb.log({'Test Loss': test_loss, **metrics, 'Epoch': epoch})\n",
    "        \n",
    "        # Checkpoint model if the test loss improved\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            model_path = f\"models/{tube_type}/model_epoch_{epoch}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            wandb.save(model_path)  # Saves the checkpoint to wandb\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, tube_type):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    differences = []\n",
    "    total_predictions = 0  # Total number of predictions\n",
    "    count_y_pred_gt_wt1 = 0  # Count of predictions greater than target\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            outputs = model(data)\n",
    "            test_loss += criterion(outputs, targets, tube_type).item()\n",
    "\n",
    "            # Increase total predictions\n",
    "            total_predictions += outputs.size(0)\n",
    "\n",
    "            if tube_type == 'sphere':\n",
    "                # Sphere-specific metric calculations\n",
    "                greater_mask = outputs > targets\n",
    "                count_y_pred_gt_wt1 += greater_mask.sum().item()\n",
    "                # Calculate differences where predictions are less than targets\n",
    "                less_mask = outputs < targets\n",
    "                differences.extend((targets[less_mask] - outputs[less_mask]).abs().tolist())\n",
    "\n",
    "            elif tube_type == 'rectangular':\n",
    "                # Rectangular-specific metric calculations\n",
    "                targets = targets.squeeze(1) # again... singular braincell human being...\n",
    "                outputs_x_norm = outputs[:, 0]\n",
    "                outputs_y_norm = outputs[:, 1]\n",
    "                targets_x_norm = targets[:, 0]\n",
    "                targets_y_norm = targets[:, 1]\n",
    "                \n",
    "                # Check if either x or y prediction is greater than the target\n",
    "                greater_mask_x = outputs_x_norm > targets_x_norm\n",
    "                greater_mask_y = outputs_y_norm > targets_y_norm\n",
    "                count_y_pred_gt_wt1 += (greater_mask_x | greater_mask_y).sum().item()\n",
    "\n",
    "                # Calculate differences for x and y separately where predictions are less than targets\n",
    "                differences_x = (targets_x_norm - outputs_x_norm)[outputs_x_norm < targets_x_norm].abs().tolist()\n",
    "                differences_y = (targets_y_norm - outputs_y_norm)[outputs_y_norm < targets_y_norm].abs().tolist()\n",
    "                differences.extend(differences_x)\n",
    "                differences.extend(differences_y)\n",
    "\n",
    "    avg_diff = sum(differences) / len(differences) if differences else 0\n",
    "    test_loss /= len(test_loader)\n",
    "    proportion_y_pred_gt_wt1 = count_y_pred_gt_wt1 / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'Test Loss': test_loss,\n",
    "        'Proportion y_pred > w_{t+1}': proportion_y_pred_gt_wt1,\n",
    "        'Avg Abs Diff y_pred < w_{t+1}': avg_diff\n",
    "    }\n",
    "    return test_loss, metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2cb56ce5",
   "metadata": {},
   "source": [
    "def main():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "    tube_type = config.tube_type\n",
    "    filename = 'processed_trajectory_data.csv'\n",
    "    \n",
    "    X, y = load_and_prepare_data(filename, tube_type)\n",
    "    train_loader, test_loader = create_data_loaders(X, y, batch_size=64)\n",
    "\n",
    "    # Set input size based on tube type\n",
    "    input_size = 6\n",
    "    output_dim = 2 if tube_type == 'rectangular' else 1  # Set output dimension based on tube type\n",
    "\n",
    "    model = TubeWidthPredictor(input_size=input_size, num_units=config.num_units, num_layers=config.num_layers, output_dim=output_dim)\n",
    "    criterion = AsymmetricLoss(alpha=config.alpha, delta=1.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    train_and_test(model, criterion, optimizer, train_loader, test_loader, tube_type, num_epochs=config.num_epochs)\n",
    "\n",
    "    wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0673ddc3",
   "metadata": {},
   "source": [
    "# Hyperparameter sweep and project setup\n",
    "alpha_values = [0.8]\n",
    "tube_types = ['rectangular', 'sphere']\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for tube_type in tube_types:\n",
    "        sweep_config = {\n",
    "            'method': 'grid',\n",
    "            'metric': {'name': 'Test Loss', 'goal': 'minimize'},\n",
    "            'parameters': {\n",
    "                'alpha': {'value': alpha},\n",
    "                'learning_rate': {'values': [0.001]},\n",
    "                'num_units': {'values': [32]},\n",
    "                'num_layers': {'values': [2]},\n",
    "                'tube_type': {'value': tube_type},\n",
    "                'num_epochs': {'value': 500}  # Example of defining epoch in config\n",
    "            }\n",
    "        }\n",
    "        project_name = f\"tube_width_experiment_alpha_{alpha}_{tube_type}\"\n",
    "        sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "        wandb.agent(sweep_id, main)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00ce65c2",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
