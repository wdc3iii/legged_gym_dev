{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in dataset: dict_keys(['z', 'pz_x', 'v', 'z_p1', 'pz_x_p1'])\n",
      "Data processing and saving complete. Data saved to rom_tracking_data/fxqv2jkf/processed_data_with_errors.pickle\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "def load_and_process_data(run_id):\n",
    "    file_path = f\"rom_tracking_data/{run_id}/dataset.pickle\"\n",
    "\n",
    "    # Load data from the pickle file\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        epoch_data = pickle.load(f)\n",
    "        print(\"Keys in dataset:\", epoch_data.keys())\n",
    "\n",
    "    processed_data = {}\n",
    "    for key in epoch_data:\n",
    "        processed_data[key] = epoch_data[key][:-1, :]\n",
    "\n",
    "    processed_data['e'] = processed_data['pz_x'] - processed_data['z']\n",
    "    processed_data['e_p1'] = np.roll(processed_data['e'], -1, axis=0)\n",
    "    processed_data['e_p1'][-1, :] = 0 # for wrap around...\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def save_data(run_id, data):\n",
    "    output_path = Path('rom_tracking_data', run_id, \"processed_data_with_errors.pickle\")\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Data processing and saving complete. Data saved to {output_path}\")\n",
    "\n",
    "# Usage example\n",
    "run_id = \"fxqv2jkf\"\n",
    "data = load_and_process_data(run_id)\n",
    "save_data(run_id, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor_input(data):\n",
    "    features = torch.tensor(data['e'], dtype=torch.float32)\n",
    "    targets = torch.tensor(data['e_p1'], dtype=torch.float32)\n",
    "    return features, targets\n",
    "\n",
    "def create_data_loaders(X, y, batch_size=64):\n",
    "    dataset = TensorDataset(X, y)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Dynamics Model\n",
    "class ErrorDynamicsModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_units, num_layers):\n",
    "        super(ErrorDynamicsModel, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, num_units), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.extend([nn.Linear(num_units, num_units), nn.ReLU()])\n",
    "        layers.append(nn.Linear(num_units, input_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Custom Loss Function for Error Dynamics\n",
    "class CustomErrorDynamicsLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomErrorDynamicsLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.norm(y_pred - y_true, p=2, dim=1).mean()\n",
    "\n",
    "# Training and Testing Routine\n",
    "def train_and_test(model, criterion, optimizer, train_loader, test_loader, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for data, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in test_loader:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        # Logging to wandb\n",
    "        wandb.log({'Epoch': epoch, 'Train Loss': total_train_loss / len(train_loader), 'Test Loss': total_test_loss / len(test_loader)})\n",
    "\n",
    "        # Save the best model\n",
    "        if epoch == 0 or total_test_loss < best_test_loss:\n",
    "            best_test_loss = total_test_loss\n",
    "            model_path = f\"models/model_epoch_{epoch}.pth\"\n",
    "            os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Main Function\n",
    "def main(run_id, num_units=100, num_layers=2, num_epochs=500):\n",
    "    wandb.login(key=\"70954bb73c536b7f5b23ef315c7c19b511e8a406\")\n",
    "    wandb.init()\n",
    "    \n",
    "    data = load_and_process_data(run_id)\n",
    "    X, y = convert_to_tensor_input(data)\n",
    "    train_loader, test_loader = create_data_loaders(X, y)\n",
    "\n",
    "    input_dim = X.size(1)\n",
    "    model = ErrorDynamicsModel(input_dim, num_units, num_layers)\n",
    "    criterion = CustomErrorDynamicsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_and_test(model, criterion, optimizer, train_loader, test_loader, num_epochs)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/colejohnson/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/colejohnson/Desktop/legged_gym_dev/deep_tube_learning/wandb/run-20240709_110312-tv8jypr6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/legged_gym_dev-deep_tube_learning/runs/tv8jypr6' target=\"_blank\">rich-snow-1</a></strong> to <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/legged_gym_dev-deep_tube_learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/legged_gym_dev-deep_tube_learning' target=\"_blank\">https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/legged_gym_dev-deep_tube_learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/legged_gym_dev-deep_tube_learning/runs/tv8jypr6' target=\"_blank\">https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/legged_gym_dev-deep_tube_learning/runs/tv8jypr6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in dataset: dict_keys(['z', 'pz_x', 'v', 'z_p1', 'pz_x_p1'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "main('fxqv2jkf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--run_id\", type=str, required=True, help=\"Run ID for the data.\")\n",
    "    parser.add_argument(\"--num_units\", type=int, default=100, help=\"Number of units in each hidden layer.\")\n",
    "    parser.add_argument(\"--num_layers\", type=int, default=2, help=\"Number of hidden layers.\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=50, help=\"Number of epochs to train.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.run_id, args.num_units, args.num_layers, args.num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
