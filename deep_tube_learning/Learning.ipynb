{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395a2a6b",
   "metadata": {},
   "source": [
    "# Tube Learning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455d9ab",
   "metadata": {},
   "source": [
    "## DataFrame Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bec8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47fc7dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely evaluate lists\n",
    "def safe_eval(col):\n",
    "    try:\n",
    "        return ast.literal_eval(col)\n",
    "    except ValueError:\n",
    "        return col  # Return as is if it's not a string representation of a list\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Set the number of robots you want to include\n",
    "num_robots = 5  # Set the number of robots to include\n",
    "robot_indices = list(range(num_robots))  # Generates a list [0, 1, 2, ..., num_robots-1]\n",
    "\n",
    "# Use glob to find all the files that match the pattern\n",
    "file_list = glob.glob('data/trajectory_data_*.csv')\n",
    "\n",
    "# Loop through the files sorted to maintain the order\n",
    "for filename in sorted(file_list):\n",
    "    temp_df = pd.read_csv(filename)\n",
    "    # Filter the DataFrame to only include rows where the robot_index is in the list of desired indices\n",
    "    temp_df = temp_df[temp_df['robot_index'].isin(robot_indices)]\n",
    "    # Apply transformations right after reading\n",
    "    temp_df['joint_positions'] = temp_df['joint_positions'].apply(safe_eval)\n",
    "    temp_df['joint_velocities'] = temp_df['joint_velocities'].apply(safe_eval)\n",
    "    all_data = pd.concat([all_data, temp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d36b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the derived columns\n",
    "all_data['x_t'] = all_data.apply(lambda row: row['joint_positions'] + row['joint_velocities'], axis=1)\n",
    "all_data['u_t'] = all_data.apply(lambda row: [row['velocity_x'], row['velocity_y']], axis=1)\n",
    "all_data['z_t'] = all_data.apply(lambda row: [row['traj_x'], row['traj_y']], axis=1)\n",
    "all_data['v_t'] = all_data.apply(lambda row: [row['reduced_command_x'], row['reduced_command_y']], axis=1)\n",
    "\n",
    "# Calculate the vector differences without norm for w_xy_t and shift it for w_xy_{t+1}\n",
    "all_data['w_xy_t'] = all_data.apply(lambda row: [row['position_x'] - row['traj_x'], row['position_y'] - row['traj_y']], axis=1)\n",
    "all_data['group'] = all_data['episode_number'].astype(str) + '_' + all_data['robot_index'].astype(str)\n",
    "\n",
    "# Original calculation for w_t for reference\n",
    "all_data['w_t'] = np.sqrt((all_data['position_x'] - all_data['traj_x'])**2 + (all_data['position_y'] - all_data['traj_y'])**2)\n",
    "\n",
    "# Shift operations for next timestep values\n",
    "all_data['x_{t+1}'] = all_data.groupby('group')['x_t'].shift(-1)\n",
    "all_data['z_{t+1}'] = all_data.groupby('group')['z_t'].shift(-1)\n",
    "all_data['w_{t+1}'] = all_data.groupby('group')['w_t'].shift(-1)\n",
    "all_data['w_xy_{t+1}'] = all_data.groupby('group')['w_xy_t'].shift(-1)\n",
    "\n",
    "# Function to drop the first and last 10 data points from each episode\n",
    "def drop_edges(group):\n",
    "    return group.iloc[10:-10]\n",
    "all_data = all_data.groupby('group', group_keys=False).apply(drop_edges)\n",
    "\n",
    "# Drop rows where x_{t+1}, z_{t+1}, w_{t+1}, and w_xy_{t+1} do not exist\n",
    "all_data.dropna(subset=['x_{t+1}', 'z_{t+1}', 'w_{t+1}', 'w_xy_{t+1}'], inplace=True)\n",
    "\n",
    "# Select and order the final columns\n",
    "final_df = all_data[['group', 'x_t', 'u_t', 'z_t', 'v_t', 'w_t', 'w_xy_t', 'x_{t+1}', 'z_{t+1}', 'w_{t+1}', 'w_xy_{t+1}']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efb734",
   "metadata": {},
   "source": [
    "We have $D=\\{\\omega_t, x_t, u_t, z_t, v_t, \\omega_{t+1}, x_{t+1}, z_{t+1}\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30dbdf51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>x_t</th>\n",
       "      <th>u_t</th>\n",
       "      <th>z_t</th>\n",
       "      <th>v_t</th>\n",
       "      <th>w_t</th>\n",
       "      <th>w_xy_t</th>\n",
       "      <th>x_{t+1}</th>\n",
       "      <th>z_{t+1}</th>\n",
       "      <th>w_{t+1}</th>\n",
       "      <th>w_xy_{t+1}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[0.021700723096728325, 0.5511646866798401, -0....</td>\n",
       "      <td>[0.5442334846271863, -0.283746258114005]</td>\n",
       "      <td>[0.0839964397055372, -0.0443801499923683]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>[-0.0259041007415884, 0.0031106175297319003]</td>\n",
       "      <td>[0.023384274914860725, 0.5582820773124695, -0....</td>\n",
       "      <td>[0.0923960836760909, -0.0488181649916052]</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>[-0.029268851125635, 0.005218078390838196]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[0.023384274914860725, 0.5582820773124695, -0....</td>\n",
       "      <td>[0.5480042460848393, -0.2824143952132594]</td>\n",
       "      <td>[0.0923960836760909, -0.0488181649916052]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>[-0.029268851125635, 0.005218078390838196]</td>\n",
       "      <td>[0.02588886395096779, 0.5638883709907532, -0.7...</td>\n",
       "      <td>[0.1007957276466446, -0.053256179990842]</td>\n",
       "      <td>0.033340</td>\n",
       "      <td>[-0.032551680733351795, 0.0072078736123690965]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[0.02588886395096779, 0.5638883709907532, -0.7...</td>\n",
       "      <td>[0.550880471042393, -0.2824574469904211]</td>\n",
       "      <td>[0.1007957276466446, -0.053256179990842]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.033340</td>\n",
       "      <td>[-0.032551680733351795, 0.0072078736123690965]</td>\n",
       "      <td>[0.028353633359074593, 0.5677175521850586, -0....</td>\n",
       "      <td>[0.1091953716171983, -0.0576941949900788]</td>\n",
       "      <td>0.036844</td>\n",
       "      <td>[-0.0357105226042108, 0.0090702395005937]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[0.028353633359074593, 0.5677175521850586, -0....</td>\n",
       "      <td>[0.5525190403579519, -0.282065262418896]</td>\n",
       "      <td>[0.1091953716171983, -0.0576941949900788]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.036844</td>\n",
       "      <td>[-0.0357105226042108, 0.0090702395005937]</td>\n",
       "      <td>[0.030368400737643242, 0.5698843002319336, -0....</td>\n",
       "      <td>[0.1175950155877521, -0.0621322099893157]</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>[-0.0387335787472045, 0.0107260937201605]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[0.030368400737643242, 0.5698843002319336, -0....</td>\n",
       "      <td>[0.5536589812999309, -0.2793862519163999]</td>\n",
       "      <td>[0.1175950155877521, -0.0621322099893157]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>[-0.0387335787472045, 0.0107260937201605]</td>\n",
       "      <td>[0.031975340098142624, 0.5703613758087158, -0....</td>\n",
       "      <td>[0.1259946595583058, -0.0665702249885525]</td>\n",
       "      <td>0.043377</td>\n",
       "      <td>[-0.041641720871798904, 0.012145407415824198]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group                                                x_t  \\\n",
       "50  1.0_0  [0.021700723096728325, 0.5511646866798401, -0....   \n",
       "55  1.0_0  [0.023384274914860725, 0.5582820773124695, -0....   \n",
       "60  1.0_0  [0.02588886395096779, 0.5638883709907532, -0.7...   \n",
       "65  1.0_0  [0.028353633359074593, 0.5677175521850586, -0....   \n",
       "70  1.0_0  [0.030368400737643242, 0.5698843002319336, -0....   \n",
       "\n",
       "                                          u_t  \\\n",
       "50   [0.5442334846271863, -0.283746258114005]   \n",
       "55  [0.5480042460848393, -0.2824143952132594]   \n",
       "60   [0.550880471042393, -0.2824574469904211]   \n",
       "65   [0.5525190403579519, -0.282065262418896]   \n",
       "70  [0.5536589812999309, -0.2793862519163999]   \n",
       "\n",
       "                                          z_t  \\\n",
       "50  [0.0839964397055372, -0.0443801499923683]   \n",
       "55  [0.0923960836760909, -0.0488181649916052]   \n",
       "60   [0.1007957276466446, -0.053256179990842]   \n",
       "65  [0.1091953716171983, -0.0576941949900788]   \n",
       "70  [0.1175950155877521, -0.0621322099893157]   \n",
       "\n",
       "                                          v_t       w_t  \\\n",
       "50  [0.4199822079150199, -0.2219007549217102]  0.026090   \n",
       "55  [0.4199822079150199, -0.2219007549217102]  0.029730   \n",
       "60  [0.4199822079150199, -0.2219007549217102]  0.033340   \n",
       "65  [0.4199822079150199, -0.2219007549217102]  0.036844   \n",
       "70  [0.4199822079150199, -0.2219007549217102]  0.040191   \n",
       "\n",
       "                                            w_xy_t  \\\n",
       "50    [-0.0259041007415884, 0.0031106175297319003]   \n",
       "55      [-0.029268851125635, 0.005218078390838196]   \n",
       "60  [-0.032551680733351795, 0.0072078736123690965]   \n",
       "65       [-0.0357105226042108, 0.0090702395005937]   \n",
       "70       [-0.0387335787472045, 0.0107260937201605]   \n",
       "\n",
       "                                              x_{t+1}  \\\n",
       "50  [0.023384274914860725, 0.5582820773124695, -0....   \n",
       "55  [0.02588886395096779, 0.5638883709907532, -0.7...   \n",
       "60  [0.028353633359074593, 0.5677175521850586, -0....   \n",
       "65  [0.030368400737643242, 0.5698843002319336, -0....   \n",
       "70  [0.031975340098142624, 0.5703613758087158, -0....   \n",
       "\n",
       "                                      z_{t+1}   w_{t+1}  \\\n",
       "50  [0.0923960836760909, -0.0488181649916052]  0.029730   \n",
       "55   [0.1007957276466446, -0.053256179990842]  0.033340   \n",
       "60  [0.1091953716171983, -0.0576941949900788]  0.036844   \n",
       "65  [0.1175950155877521, -0.0621322099893157]  0.040191   \n",
       "70  [0.1259946595583058, -0.0665702249885525]  0.043377   \n",
       "\n",
       "                                        w_xy_{t+1}  \n",
       "50      [-0.029268851125635, 0.005218078390838196]  \n",
       "55  [-0.032551680733351795, 0.0072078736123690965]  \n",
       "60       [-0.0357105226042108, 0.0090702395005937]  \n",
       "65       [-0.0387335787472045, 0.0107260937201605]  \n",
       "70   [-0.041641720871798904, 0.012145407415824198]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the final DataFrame\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3770d",
   "metadata": {},
   "source": [
    "Optional Saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdcd6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('processed_trajectory_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d689932",
   "metadata": {},
   "source": [
    "## Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2ec200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcoleonguard\u001b[0m (\u001b[33mcoleonguard-Georgia Institute of Technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/colejohnson/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.login(key=\"70954bb73c536b7f5b23ef315c7c19b511e8a406\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6799e",
   "metadata": {},
   "source": [
    "Load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5abf44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_eval(col):\n",
    "    try:\n",
    "        return ast.literal_eval(col)\n",
    "    except ValueError:\n",
    "        return col  # Return as is if it's not a string representation of a list\n",
    "\n",
    "def convert_to_tensor_input(row):\n",
    "    flat_list = []\n",
    "    for item in row:\n",
    "        if isinstance(item, list):\n",
    "            flat_list.extend(item)\n",
    "        else:\n",
    "            flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "def load_and_prepare_data(filename, tube_type):\n",
    "    df = pd.read_csv(filename)\n",
    "    list_columns = ['u_t', 'z_t', 'v_t'] + (['w_xy_t', 'w_xy_{t+1}'] if tube_type == 'rectangular' else [])\n",
    "    feature_columns = ['u_t', 'z_t', 'v_t']\n",
    "    \n",
    "    for col in list_columns:\n",
    "        df[col] = df[col].apply(safe_eval)  # Assumes safe_eval correctly parses strings into lists\n",
    "\n",
    "    X = torch.tensor(df[feature_columns].apply(convert_to_tensor_input, axis=1).tolist(), dtype=torch.float32)\n",
    "    \n",
    "    if tube_type == 'rectangular':\n",
    "        target_columns = ['w_xy_t', 'w_xy_{t+1}']\n",
    "    else:\n",
    "        target_columns = ['w_t', 'w_{t+1}']\n",
    "\n",
    "    # Construct the target tensor manually from rows\n",
    "    y_data = df[target_columns].apply(lambda row: [row[col] for col in target_columns], axis=1).tolist()\n",
    "    y = torch.tensor(y_data, dtype=torch.float32)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def create_data_loaders(X, y, batch_size=64):\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(X, y[:, 1].unsqueeze(1))  # Assumes the target is at the second position\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ce8e4",
   "metadata": {},
   "source": [
    "Model specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f732340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeWidthPredictor(nn.Module):\n",
    "    def __init__(self, input_size, num_units, num_layers, output_dim=1):\n",
    "        super(TubeWidthPredictor, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_size, num_units), nn.ReLU()])\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_units, num_units))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(num_units, output_dim))  # Dynamically set output dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, delta=1.0):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "\n",
    "    def forward(self, y_pred, y_true, tube_type):\n",
    "        if tube_type == 'sphere':\n",
    "            residual = y_true - y_pred\n",
    "            loss = torch.where(residual <= 0, self.alpha * residual, (1 - self.alpha) * residual.abs())\n",
    "            return self.huber(loss, torch.zeros_like(loss))\n",
    "        elif tube_type == 'rectangular':\n",
    "            # Compute L2 norm of the residuals for each component\n",
    "            y_true = y_true.squeeze(1) # because im a lazy idiot\n",
    "            residual_x = y_true[:, 0] - y_pred[:, 0]\n",
    "            residual_y = y_true[:, 1] - y_pred[:, 1]\n",
    "            norm_residual = torch.sqrt(residual_x**2 + residual_y**2)\n",
    "            loss = torch.where(norm_residual <= 0, self.alpha * norm_residual, (1 - self.alpha) * norm_residual.abs())\n",
    "            return self.huber(loss, torch.zeros_like(loss))\n",
    "\n",
    "def train_and_test(model, criterion, optimizer, train_loader, test_loader, tube_type, num_epochs=500):\n",
    "    best_test_loss = float('inf')  # Initialize with a large value\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=0, leave=True):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, targets in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets, tube_type)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Log train loss to wandb\n",
    "        wandb.log({'Train Loss': train_loss / len(train_loader), 'Epoch': epoch})\n",
    "\n",
    "        # Evaluate the model\n",
    "        test_loss, metrics = evaluate_model(model, test_loader, criterion, tube_type)\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        wandb.log({'Test Loss': test_loss, **metrics, 'Epoch': epoch})\n",
    "        \n",
    "        # Checkpoint model if the test loss improved\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            model_path = f\"models/{tube_type}/model_epoch_{epoch}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            wandb.save(model_path)  # Saves the checkpoint to wandb\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, tube_type):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    differences = []\n",
    "    total_predictions = 0  # Total number of predictions\n",
    "    count_y_pred_gt_wt1 = 0  # Count of predictions greater than target\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            outputs = model(data)\n",
    "            test_loss += criterion(outputs, targets, tube_type).item()\n",
    "\n",
    "            # Increase total predictions\n",
    "            total_predictions += outputs.size(0)\n",
    "\n",
    "            if tube_type == 'sphere':\n",
    "                # Sphere-specific metric calculations\n",
    "                greater_mask = outputs > targets\n",
    "                count_y_pred_gt_wt1 += greater_mask.sum().item()\n",
    "                # Calculate differences where predictions are less than targets\n",
    "                less_mask = outputs < targets\n",
    "                differences.extend((targets[less_mask] - outputs[less_mask]).abs().tolist())\n",
    "\n",
    "            elif tube_type == 'rectangular':\n",
    "                # Rectangular-specific metric calculations\n",
    "                targets = targets.squeeze(1) # again... singular braincell human being...\n",
    "                outputs_x_norm = outputs[:, 0]\n",
    "                outputs_y_norm = outputs[:, 1]\n",
    "                targets_x_norm = targets[:, 0]\n",
    "                targets_y_norm = targets[:, 1]\n",
    "                \n",
    "                # Check if either x or y prediction is greater than the target\n",
    "                greater_mask_x = outputs_x_norm > targets_x_norm\n",
    "                greater_mask_y = outputs_y_norm > targets_y_norm\n",
    "                count_y_pred_gt_wt1 += (greater_mask_x | greater_mask_y).sum().item()\n",
    "\n",
    "                # Calculate differences for x and y separately where predictions are less than targets\n",
    "                differences_x = (targets_x_norm - outputs_x_norm)[outputs_x_norm < targets_x_norm].abs().tolist()\n",
    "                differences_y = (targets_y_norm - outputs_y_norm)[outputs_y_norm < targets_y_norm].abs().tolist()\n",
    "                differences.extend(differences_x)\n",
    "                differences.extend(differences_y)\n",
    "\n",
    "    avg_diff = sum(differences) / len(differences) if differences else 0\n",
    "    test_loss /= len(test_loader)\n",
    "    proportion_y_pred_gt_wt1 = count_y_pred_gt_wt1 / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'Test Loss': test_loss,\n",
    "        'Proportion y_pred > w_{t+1}': proportion_y_pred_gt_wt1,\n",
    "        'Avg Abs Diff y_pred < w_{t+1}': avg_diff\n",
    "    }\n",
    "    return test_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb56ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "    tube_type = config.tube_type\n",
    "    filename = 'processed_trajectory_data.csv'\n",
    "    \n",
    "    X, y = load_and_prepare_data(filename, tube_type)\n",
    "    train_loader, test_loader = create_data_loaders(X, y, batch_size=64)\n",
    "\n",
    "    # Set input size based on tube type\n",
    "    input_size = 6\n",
    "    output_dim = 2 if tube_type == 'rectangular' else 1  # Set output dimension based on tube type\n",
    "\n",
    "    model = TubeWidthPredictor(input_size=input_size, num_units=config.num_units, num_layers=config.num_layers, output_dim=output_dim)\n",
    "    criterion = AsymmetricLoss(alpha=config.alpha, delta=1.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    train_and_test(model, criterion, optimizer, train_loader, test_loader, tube_type, num_epochs=config.num_epochs)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673ddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: vbvvzhpx\n",
      "Sweep URL: https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/sweeps/vbvvzhpx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ospsfh6v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_units: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttube_type: rectangular\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/colejohnson/Desktop/legged_gym_dev/deep_tube_learning/wandb/run-20240704_195008-ospsfh6v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/runs/ospsfh6v' target=\"_blank\">scarlet-sweep-1</a></strong> to <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/sweeps/vbvvzhpx' target=\"_blank\">https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/sweeps/vbvvzhpx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular' target=\"_blank\">https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/sweeps/vbvvzhpx' target=\"_blank\">https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/sweeps/vbvvzhpx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/runs/ospsfh6v' target=\"_blank\">https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/runs/ospsfh6v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.037 MB of 0.037 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-sweep-1</strong> at: <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/runs/ospsfh6v' target=\"_blank\">https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular/runs/ospsfh6v</a><br/> View project at: <a href='https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular' target=\"_blank\">https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_rectangular</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240704_195008-ospsfh6v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ospsfh6v errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/colejohnson/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'w_xy_{t+1}'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/colejohnson/opt/anaconda3/lib/python3.9/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/xn/h81hn6wd44q8p6gmpfy06f6w0000gn/T/ipykernel_71509/1388499124.py\", line 7, in main\n",
      "    X, y = load_and_prepare_data(filename, tube_type)\n",
      "  File \"/var/folders/xn/h81hn6wd44q8p6gmpfy06f6w0000gn/T/ipykernel_71509/3893137039.py\", line 22, in load_and_prepare_data\n",
      "    df[col] = df[col].apply(safe_eval)  # Assumes safe_eval correctly parses strings into lists\n",
      "  File \"/Users/colejohnson/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/Users/colejohnson/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'w_xy_{t+1}'\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ospsfh6v errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/colejohnson/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._engine.get_loc(casted_key)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m KeyError: 'w_xy_{t+1}'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/colejohnson/opt/anaconda3/lib/python3.9/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/xn/h81hn6wd44q8p6gmpfy06f6w0000gn/T/ipykernel_71509/1388499124.py\", line 7, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     X, y = load_and_prepare_data(filename, tube_type)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/xn/h81hn6wd44q8p6gmpfy06f6w0000gn/T/ipykernel_71509/3893137039.py\", line 22, in load_and_prepare_data\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     df[col] = df[col].apply(safe_eval)  # Assumes safe_eval correctly parses strings into lists\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/colejohnson/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     indexer = self.columns.get_loc(key)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/colejohnson/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise KeyError(key) from err\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m KeyError: 'w_xy_{t+1}'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: nom0izfo\n",
      "Sweep URL: https://wandb.ai/coleonguard-Georgia%20Institute%20of%20Technology/tube_width_experiment_alpha_0.8_sphere/sweeps/nom0izfo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3icgl591 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_units: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttube_type: sphere\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter sweep and project setup\n",
    "alpha_values = [0.8]\n",
    "tube_types = ['rectangular', 'sphere']\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for tube_type in tube_types:\n",
    "        sweep_config = {\n",
    "            'method': 'grid',\n",
    "            'metric': {'name': 'Test Loss', 'goal': 'minimize'},\n",
    "            'parameters': {\n",
    "                'alpha': {'value': alpha},\n",
    "                'learning_rate': {'values': [0.001]},\n",
    "                'num_units': {'values': [32]},\n",
    "                'num_layers': {'values': [2]},\n",
    "                'tube_type': {'value': tube_type},\n",
    "                'num_epochs': {'value': 500}  # Example of defining epoch in config\n",
    "            }\n",
    "        }\n",
    "        project_name = f\"tube_width_experiment_alpha_{alpha}_{tube_type}\"\n",
    "        sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "        wandb.agent(sweep_id, main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce65c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
