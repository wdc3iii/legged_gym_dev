{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395a2a6b",
   "metadata": {},
   "source": [
    "# Tube Learning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455d9ab",
   "metadata": {},
   "source": [
    "## DataFrame Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the concatenated CSV file into a pandas DataFrame\n",
    "filename = 'concatenated_trajectory_data.csv'\n",
    "all_data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d36b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert necessary fields into appropriate data types\n",
    "df['joint_positions'] = df['joint_positions'].apply(ast.literal_eval)\n",
    "df['joint_velocities'] = df['joint_velocities'].apply(ast.literal_eval)\n",
    "\n",
    "# Step 3: Consolidate columns into new structure\n",
    "df['x_t'] = df.apply(lambda row: row['joint_positions'] + row['joint_velocities'], axis=1)\n",
    "df['u_t'] = df.apply(lambda row: [row['velocity_x'], row['velocity_y']], axis=1)\n",
    "df['z_t'] = df.apply(lambda row: [row['traj_x'], row['traj_y']], axis=1)\n",
    "df['v_t'] = df.apply(lambda row: [row['reduced_command_x'], row['reduced_command_y']], axis=1)\n",
    "\n",
    "# Step 4: Calculate the Euclidean distance between position and trajectory for w_t\n",
    "df['w_t'] = np.sqrt((df['position_x'] - df['traj_x'])**2 + (df['position_y'] - df['traj_y'])**2)\n",
    "\n",
    "# Step 5: Create the \"group\" column\n",
    "df['group'] = df['episode_number'].astype(str) + '_' + df['robot_index'].astype(str)\n",
    "\n",
    "# Step 6: Create x_{t+1}, z_{t+1}, and w_{t+1} by shifting the DataFrame within each group\n",
    "df['x_{t+1}'] = df.groupby('group')['x_t'].shift(-1)\n",
    "df['z_{t+1}'] = df.groupby('group')['z_t'].shift(-1)\n",
    "df['w_{t+1}'] = df.groupby('group')['w_t'].shift(-1)\n",
    "\n",
    "# Drop rows where we do not have x_{t+1}, z_{t+1}, and w_{t+1}\n",
    "df.dropna(subset=['x_{t+1}', 'z_{t+1}', 'w_{t+1}'], inplace=True)\n",
    "\n",
    "# Step 7: Drop the first and last 10 data points from each episode\n",
    "def drop_edges(group):\n",
    "    return group.iloc[10:-10]\n",
    "\n",
    "df = df.groupby('group', group_keys=False).apply(drop_edges)\n",
    "\n",
    "# Step 8: Select and order the columns\n",
    "final_df = df[['group', 'x_t', 'u_t', 'z_t', 'v_t', 'w_t', 'x_{t+1}', 'z_{t+1}', 'w_{t+1}']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efb734",
   "metadata": {},
   "source": [
    "We have $D=\\{\\omega_t, x_t, u_t, z_t, v_t, \\omega_{t+1}, x_{t+1}, z_{t+1}\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbdf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final DataFrame\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3770d",
   "metadata": {},
   "source": [
    "Optional Saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('processed_trajectory_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d689932",
   "metadata": {},
   "source": [
    "## Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ec200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6799e",
   "metadata": {},
   "source": [
    "Load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (assuming DataFrame is saved in a CSV file named 'data.csv')\n",
    "final_df = pd.read_csv('processed_trajectory_data.csv')\n",
    "\n",
    "# Convert to tensors and split data\n",
    "X = torch.tensor(final_df[['x_t', 'u_t', 'z_t', 'v_t']].values.tolist(), dtype=torch.float32)\n",
    "y = torch.tensor(final_df[['w_t', 'w_{t+1}']].values, dtype=torch.float32)\n",
    "dataset = TensorDataset(X, y[:, 1].unsqueeze(1))\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ce8e4",
   "metadata": {},
   "source": [
    "Model specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f732340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeWidthPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TubeWidthPredictor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(16, 64),  # Adjust input size based on your data structure\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, delta=1.0):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        residual = y_true - y_pred\n",
    "        loss = torch.where(residual > 0, self.alpha * residual, (1 - self.alpha) * residual.abs())\n",
    "        return self.huber(loss, torch.zeros_like(loss))\n",
    "    \n",
    "def train(num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    print(f'Test Loss: {total_loss / len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb56ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TubeWidthPredictor()\n",
    "criterion = AsymmetricLoss(alpha=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter('runs/tube_width_experiment')\n",
    "train(50)\n",
    "test()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
