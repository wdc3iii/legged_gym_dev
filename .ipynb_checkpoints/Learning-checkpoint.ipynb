{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395a2a6b",
   "metadata": {},
   "source": [
    "# Tube Learning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455d9ab",
   "metadata": {},
   "source": [
    "## DataFrame Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4bec8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eec978eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely evaluate lists\n",
    "def safe_eval(col):\n",
    "    try:\n",
    "        return ast.literal_eval(col)\n",
    "    except ValueError:\n",
    "        return col  # Return as is if it's not a string representation of a list\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Use glob to find all the files that match the pattern\n",
    "file_list = glob.glob('data/trajectory_data_*.csv')\n",
    "\n",
    "# Loop through the files sorted to maintain the order\n",
    "for filename in sorted(file_list):\n",
    "    temp_df = pd.read_csv(filename)\n",
    "    # Apply transformations right after reading\n",
    "    temp_df['joint_positions'] = temp_df['joint_positions'].apply(safe_eval)\n",
    "    temp_df['joint_velocities'] = temp_df['joint_velocities'].apply(safe_eval)\n",
    "    all_data = pd.concat([all_data, temp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b7d36b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the derived columns\n",
    "all_data['x_t'] = all_data.apply(lambda row: row['joint_positions'] + row['joint_velocities'], axis=1)\n",
    "all_data['u_t'] = all_data.apply(lambda row: [row['velocity_x'], row['velocity_y']], axis=1)\n",
    "all_data['z_t'] = all_data.apply(lambda row: [row['traj_x'], row['traj_y']], axis=1)\n",
    "all_data['v_t'] = all_data.apply(lambda row: [row['reduced_command_x'], row['reduced_command_y']], axis=1)\n",
    "\n",
    "\n",
    "# Since w_t and w_{t+1} are derived from calculations, no need for safe_eval\n",
    "all_data['w_t'] = np.sqrt((all_data['position_x'] - all_data['traj_x'])**2 + (all_data['position_y'] - all_data['traj_y'])**2)\n",
    "all_data['group'] = all_data['episode_number'].astype(str) + '_' + all_data['robot_index'].astype(str)\n",
    "\n",
    "# Example to debug with a smaller subset\n",
    "all_data['x_{t+1}'] = all_data.groupby('group')['x_t'].shift(-1)\n",
    "all_data['z_{t+1}'] = all_data.groupby('group')['z_t'].shift(-1)\n",
    "all_data['w_{t+1}'] = all_data.groupby('group')['w_t'].shift(-1)\n",
    "\n",
    "# Function to drop the first and last 10 data points from each episode\n",
    "def drop_edges(group):\n",
    "    return group.iloc[1:-1]\n",
    "all_data = all_data.groupby('group', group_keys=False).apply(drop_edges)\n",
    "\n",
    "# Drop rows where x_{t+1}, z_{t+1}, and w_{t+1} do not exist\n",
    "all_data.dropna(subset=['x_{t+1}', 'z_{t+1}', 'w_{t+1}'], inplace=True)\n",
    "\n",
    "# Select and order the final columns\n",
    "final_df = all_data[['group', 'x_t', 'u_t', 'z_t', 'v_t', 'w_t', 'x_{t+1}', 'z_{t+1}', 'w_{t+1}']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efb734",
   "metadata": {},
   "source": [
    "We have $D=\\{\\omega_t, x_t, u_t, z_t, v_t, \\omega_{t+1}, x_{t+1}, z_{t+1}\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30dbdf51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>x_t</th>\n",
       "      <th>u_t</th>\n",
       "      <th>z_t</th>\n",
       "      <th>v_t</th>\n",
       "      <th>w_t</th>\n",
       "      <th>x_{t+1}</th>\n",
       "      <th>z_{t+1}</th>\n",
       "      <th>w_{t+1}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[-0.016817141324281693, 0.41679614782333374, -...</td>\n",
       "      <td>[0.499429464331994, -0.0896594062640303]</td>\n",
       "      <td>[0.0083996439705537, -0.0044380149992368]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>[-0.023087259382009506, 0.4356231689453125, -0...</td>\n",
       "      <td>[0.0167992879411074, -0.0088760299984736]</td>\n",
       "      <td>0.015988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[-0.023087259382009506, 0.4356231689453125, -0...</td>\n",
       "      <td>[0.4589464469104678, -0.1296557248570755]</td>\n",
       "      <td>[0.0167992879411074, -0.0088760299984736]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.015988</td>\n",
       "      <td>[-0.0035673792008310556, 0.4758116602897644, -...</td>\n",
       "      <td>[0.0251989319116611, -0.0133140449977105]</td>\n",
       "      <td>0.015224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[-0.0035673792008310556, 0.4758116602897644, -...</td>\n",
       "      <td>[0.4631590609554294, -0.2217794437849829]</td>\n",
       "      <td>[0.0251989319116611, -0.0133140449977105]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.015224</td>\n",
       "      <td>[0.008097478188574314, 0.4927358627319336, -0....</td>\n",
       "      <td>[0.0335985758822148, -0.0177520599969473]</td>\n",
       "      <td>0.013430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[0.008097478188574314, 0.4927358627319336, -0....</td>\n",
       "      <td>[0.5037832817056503, -0.2699768350131482]</td>\n",
       "      <td>[0.0335985758822148, -0.0177520599969473]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.013430</td>\n",
       "      <td>[0.014785615727305412, 0.5032246708869934, -0....</td>\n",
       "      <td>[0.0419982198527686, -0.0221900749961841]</td>\n",
       "      <td>0.012822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>1.0_0</td>\n",
       "      <td>[0.014785615727305412, 0.5032246708869934, -0....</td>\n",
       "      <td>[0.5225719853098, -0.2828309319952543]</td>\n",
       "      <td>[0.0419982198527686, -0.0221900749961841]</td>\n",
       "      <td>[0.4199822079150199, -0.2219007549217102]</td>\n",
       "      <td>0.012822</td>\n",
       "      <td>[0.01734859123826027, 0.514514148235321, -0.73...</td>\n",
       "      <td>[0.0503978638233223, -0.026628089995421]</td>\n",
       "      <td>0.014061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group                                                x_t  \\\n",
       "1000  1.0_0  [-0.016817141324281693, 0.41679614782333374, -...   \n",
       "2000  1.0_0  [-0.023087259382009506, 0.4356231689453125, -0...   \n",
       "3000  1.0_0  [-0.0035673792008310556, 0.4758116602897644, -...   \n",
       "4000  1.0_0  [0.008097478188574314, 0.4927358627319336, -0....   \n",
       "5000  1.0_0  [0.014785615727305412, 0.5032246708869934, -0....   \n",
       "\n",
       "                                            u_t  \\\n",
       "1000   [0.499429464331994, -0.0896594062640303]   \n",
       "2000  [0.4589464469104678, -0.1296557248570755]   \n",
       "3000  [0.4631590609554294, -0.2217794437849829]   \n",
       "4000  [0.5037832817056503, -0.2699768350131482]   \n",
       "5000     [0.5225719853098, -0.2828309319952543]   \n",
       "\n",
       "                                            z_t  \\\n",
       "1000  [0.0083996439705537, -0.0044380149992368]   \n",
       "2000  [0.0167992879411074, -0.0088760299984736]   \n",
       "3000  [0.0251989319116611, -0.0133140449977105]   \n",
       "4000  [0.0335985758822148, -0.0177520599969473]   \n",
       "5000  [0.0419982198527686, -0.0221900749961841]   \n",
       "\n",
       "                                            v_t       w_t  \\\n",
       "1000  [0.4199822079150199, -0.2219007549217102]  0.013292   \n",
       "2000  [0.4199822079150199, -0.2219007549217102]  0.015988   \n",
       "3000  [0.4199822079150199, -0.2219007549217102]  0.015224   \n",
       "4000  [0.4199822079150199, -0.2219007549217102]  0.013430   \n",
       "5000  [0.4199822079150199, -0.2219007549217102]  0.012822   \n",
       "\n",
       "                                                x_{t+1}  \\\n",
       "1000  [-0.023087259382009506, 0.4356231689453125, -0...   \n",
       "2000  [-0.0035673792008310556, 0.4758116602897644, -...   \n",
       "3000  [0.008097478188574314, 0.4927358627319336, -0....   \n",
       "4000  [0.014785615727305412, 0.5032246708869934, -0....   \n",
       "5000  [0.01734859123826027, 0.514514148235321, -0.73...   \n",
       "\n",
       "                                        z_{t+1}   w_{t+1}  \n",
       "1000  [0.0167992879411074, -0.0088760299984736]  0.015988  \n",
       "2000  [0.0251989319116611, -0.0133140449977105]  0.015224  \n",
       "3000  [0.0335985758822148, -0.0177520599969473]  0.013430  \n",
       "4000  [0.0419982198527686, -0.0221900749961841]  0.012822  \n",
       "5000   [0.0503978638233223, -0.026628089995421]  0.014061  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the final DataFrame\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "83b2aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\n"
     ]
    }
   ],
   "source": [
    "# Count the number of data points in each group\n",
    "group_sizes = all_data.groupby('group').size()\n",
    "print(group_sizes.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3770d",
   "metadata": {},
   "source": [
    "Optional Saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdcd6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('processed_trajectory_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d689932",
   "metadata": {},
   "source": [
    "## Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df2ec200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 11:20:33.706095: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6799e",
   "metadata": {},
   "source": [
    "Load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5abf44b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_trajectory_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert to tensors and split data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mu_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(final_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_t\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mt+1}\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(X, y[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# Load data (assuming DataFrame is saved in a CSV file named 'data.csv')\n",
    "final_df = pd.read_csv('processed_trajectory_data.csv')\n",
    "\n",
    "# Convert to tensors and split data\n",
    "X = torch.tensor(final_df[['x_t', 'u_t', 'z_t', 'v_t']].values.tolist(), dtype=torch.float32)\n",
    "y = torch.tensor(final_df[['w_t', 'w_{t+1}']].values, dtype=torch.float32)\n",
    "dataset = TensorDataset(X, y[:, 1].unsqueeze(1))\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ce8e4",
   "metadata": {},
   "source": [
    "Model specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f732340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeWidthPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TubeWidthPredictor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(16, 64),  # Adjust input size based on your data structure\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, delta=1.0):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        residual = y_true - y_pred\n",
    "        loss = torch.where(residual > 0, self.alpha * residual, (1 - self.alpha) * residual.abs())\n",
    "        return self.huber(loss, torch.zeros_like(loss))\n",
    "    \n",
    "def train(num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    print(f'Test Loss: {total_loss / len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb56ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TubeWidthPredictor()\n",
    "criterion = AsymmetricLoss(alpha=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter('runs/tube_width_experiment')\n",
    "train(50)\n",
    "test()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
